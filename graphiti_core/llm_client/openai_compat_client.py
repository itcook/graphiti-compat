"""
Copyright 2024, Zep Software, Inc.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
"""

import json
import typing

from pydantic import BaseModel

from ..prompts.models import Message
from .client import MULTILINGUAL_EXTRACTION_RESPONSES
from .config import ModelSize
from .openai_generic_client import OpenAIGenericClient


class OpenAICompatClient(OpenAIGenericClient):
    """
    OpenAI API å…¼å®¹å®¢æˆ·ç«¯ï¼Œæ”¯æŒ DeepSeek ç­‰å…¼å®¹ OpenAI API çš„å¤§æ¨¡å‹

    ä¸»è¦å·®å¼‚ï¼šä¿®æ”¹ JSON å“åº”æç¤ºè¯æ ¼å¼ï¼Œé¿å…æŸäº›æ¨¡å‹ç›´æ¥è¿”å›æ¨¡æ¿å†…å®¹
    """

    async def generate_response(
        self,
        messages: list[Message],
        response_model: type[BaseModel] | None = None,
        max_tokens: int | None = None,
        model_size: ModelSize = ModelSize.medium,
    ) -> dict[str, typing.Any]:
        if max_tokens is None:
            max_tokens = self.max_tokens

        retry_count = 0
        last_error = None

        if response_model is not None:
            serialized_model = json.dumps(response_model.model_json_schema())
            # å…³é”®å·®å¼‚ï¼šä¿®æ”¹æç¤ºè¯æ ¼å¼ï¼Œé¿å…LLMè¿”å›JSON schema
            json_instruction=f"\n\nRespond with a JSON object that follows the JSON schema instruction:\n\n{serialized_model}\n\nIMPORTANT: The output JSON object should not be the instruction itself. Instead, the instruction describes the structure of the JSON object that you must generate with actual data."
            messages[-1].content += json_instruction

        # æ·»åŠ å¤šè¯­è¨€æå–æŒ‡ä»¤
        messages[0].content += MULTILINGUAL_EXTRACTION_RESPONSES

        # è°ƒè¯•ï¼šæ‰“å°å‘é€ç»™LLMçš„å®Œæ•´æ¶ˆæ¯
        import logging
        logger = logging.getLogger(__name__)
        logger.info("ğŸ” Sending messages to LLM (OpenAICompatClient):")
        for i, msg in enumerate(messages):
            logger.info(f"  Message {i+1} ({msg.role}): {msg.content[:500]}...")
            if len(msg.content) > 500:
                logger.info(f"    [Message truncated, full length: {len(msg.content)} chars]")
                # æ‰“å°æ¶ˆæ¯çš„æœ€å500ä¸ªå­—ç¬¦ï¼ŒæŸ¥çœ‹æˆ‘ä»¬çš„ä¼˜åŒ–æç¤ºè¯
                logger.info(f"    Message {i+1} ending: ...{msg.content[-500:]}")

        # é‡è¯•é€»è¾‘ï¼ˆå®Œå…¨è¦†å†™ï¼Œé¿å…çˆ¶ç±»é‡æ–°æ·»åŠ åŸæ¥çš„æç¤ºè¯æ ¼å¼ï¼‰
        while retry_count <= self.MAX_RETRIES:
            try:
                response = await self._generate_response(
                    messages, response_model, max_tokens=max_tokens, model_size=model_size
                )
                logger.info(f"ğŸ¤– LLM Response (OpenAICompatClient): {response}")
                return response
            except Exception as e:
                last_error = e

                # Don't retry if we've hit the max retries
                if retry_count >= self.MAX_RETRIES:
                    logger.error(f'Max retries ({self.MAX_RETRIES}) exceeded. Last error: {e}')
                    raise

                retry_count += 1

                # Construct a detailed error message for the LLM
                error_context = (
                    f'The previous response attempt was invalid. '
                    f'Error type: {e.__class__.__name__}. '
                    f'Error details: {str(e)}. '
                    f'Please try again with a valid response, ensuring the output matches '
                    f'the expected format and constraints.'
                )

                error_message = Message(role='user', content=error_context)
                messages.append(error_message)
                logger.warning(
                    f'Retrying after application error (attempt {retry_count}/{self.MAX_RETRIES}): {e}'
                )

        # If we somehow get here, raise the last error
        raise last_error or Exception('Max retries exceeded with no specific error')
